import pickle

# 저장할 파일 경로
pickle_file_path = "re_cv_e.pickle"

# re_cv_e 객체를 피클 파일로 저장
with open(pickle_file_path, 'wb') as pickle_file:
    pickle.dump(re_cv_e, pickle_file,pickle.HIGHEST_PROTOCOL)

print("re_cv_e를 피클 파일로 저장했습니다.")


import pickle

pickle_file_path = "/Users/kimmirae/Desktop/DIMA3/2project/re_cv_e.pickle"

# 파일을 바이너리 모드로 열고 pickle 객체를 직접 로드
with open(pickle_file_path, 'rb') as pickle_file:
    #re_cv_e_loaded = pickle.load(pickle_file)
    re_cv_e = pickle.load(pickle_file)


# 저장된 피클 파일을 로드하여 dense_matrix_e 데이터프레임 복원
pickle_file_path = "dense_matrix.pickle"
with open(pickle_file_path, 'rb') as pickle_file:
    dense_matrix = pickle.load(pickle_file)


# df를 피클로 저장 

#피클 파일로 저장하기 

import pickle

# 저장할 파일 경로
pickle_file_path = "dataframe.pickle"

# dense_matrix_e 데이터프레임을 피클 파일로 저장
with open(pickle_file_path, 'wb') as pickle_file:
    pickle.dump(df, pickle_file,pickle.HIGHEST_PROTOCOL)

print("df를 피클 파일로 저장했습니다.")


pickle_file_path = "dataframe.pickle"
with open(pickle_file_path, 'rb') as pickle_file:
    df = pickle.load(pickle_file)


dense_matrix





import pickle

# 1. 카운트 벡터라이즈 모댈 불러오기 
pickle_file_path = "re_cv_e.pickle"
with open(pickle_file_path, 'rb') as pickle_file:
    #re_cv_e_loaded = pickle.load(pickle_file)
    re_cv_e = pickle.load(pickle_file)

# 2. 회사 크롤링 데이터 불러오기 
pickle_file_path = "dataframe.pickle"
with open(pickle_file_path, 'rb') as pickle_file:
    df = pickle.load(pickle_file)

# 3. 각 회사별 키워트 카운트한 데이터 프레임 불러오기 
pickle_file_path = "dense_matrix.pickle"
with open(pickle_file_path, 'rb') as pickle_file:
    dense_matrix = pickle.load(pickle_file)


import pickle

def countVectorPkl()
    # 1. 카운트 벡터라이즈 모댈 불러오기 
    pickle_file_path = "re_cv_e.pickle"
    with open(pickle_file_path, 'rb') as pickle_file:
        #re_cv_e_loaded = pickle.load(pickle_file)
        #re_cv_e = pickle.load(pickle_file)
    return pickle.load(pickle_file)
    
    # 2. 회사 크롤링 데이터 불러오기 
def dataFramePkl()
    pickle_file_path = "dataframe.pickle"
    with open(pickle_file_path, 'rb') as pickle_file:
        #df = pickle.load(pickle_file)
    return pickle.load(pickle_file)

def dens_matrixPkl()
    # 3. 각 회사별 키워트 카운트한 데이터 프레임 불러오기 
    pickle_file_path = "dense_matrix.pickle"
    with open(pickle_file_path, 'rb') as pickle_file:
        #dense_matrix = pickle.load(pickle_file)
    return pickle.load(pickle_file)


from nltk import word_tokenize
from nltk import sent_tokenize
from nltk import pos_tag
import nltk
import re
from nltk import WordNetLemmatizer
from konlpy.tag import Mecab
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import numpy as np

wlem = nltk.WordNetLemmatizer()
tokenizer = Mecab().morphs

# 한글 및 영어 텍스트를 문자열로 변경 + 쉼표 전처리 함수 지정
def to_string(text): # 리스트에서 문자열로 다 바꿔주기 
    text = str(text)
    text=text.replace("'","")
    text=text.replace("[","")
    text=text.replace("]","") 
    return text

def calculate_similarity(input_vector, df_vectors):
    similarity = cosine_similarity(input_vector, df_vectors)
    return similarity[0]

def inputKeyword(input_keyword):
    input_keyword = input_keyword.lower() # 리스트-> 문자열로 바꿈 

    # 불용어 제거 및 단어만 추출 
    stopwords = nltk.corpus.stopwords.words('english') # 영어 불용어 
    all_tokens = []
    
    word_tokens = word_tokenize(input_keyword) # 단어로 토큰화 -> 리스트로 반환

    for word in word_tokens:
        if len(word) != 1: # 글자수 1이상만 출력 
            if word not in stopwords: # 불용어를 포함하지 않는다면 그대로 추가하기
                all_tokens.append(word)
    
    #stopword 제거
    english_return = []
    tagged_sentence = pos_tag(all_tokens) # 명사로 바꿔주려고

    for i in range(len(tagged_sentence)):
        if tagged_sentence[i][1].startswith('N'): # 명사형
            new_word = wlem.lemmatize(tagged_sentence[i][0])   #명사의 원형으로 바꿔서 넣어주기    
            english_return.append(new_word)
            
    input_keyword = english_return
    
    count = 0
    search_keyword = []
    
    for word in input_keyword:
        if word in re_cv_e.get_feature_names_out(): # 입력한 키워드가 없다면 품목리스트에 없을 경우 대비 
            search_keyword.append(word)
            count +=1
        else:
            continue
    if count == 0: # 입력한 키워드가 품목리스트에 하나도 없음  
        print('입력하신 키워드는 품목리스트에 없습니다. 다시 입력해주세요')
        return None;
        
    else: # 입력한 키워드가 품목리스트에 있다면 
        print(f'"{str(search_keyword)[2:-2]}" 취급하는 회사 url에 대한 검색')
        print('\n')
        input_keyword = to_string(input_keyword) # 문자열로 바꿔주는 작업
        buyer = pd.DataFrame([input_keyword], columns=['order'])
        buyer1 = re_cv_e.transform(buyer['order'])
        dense = pd.DataFrame(buyer1.toarray(), columns=re_cv_e.get_feature_names_out())
        #display(dense)
        #alldense_matrix = dense_matrix # 
        columns_index =  dense_matrix.index
        #break

        ## 코사인 유사도 구함 
        similarity  = calculate_similarity(dense,dense_matrix) #사용자 입력과 각 url별과 코사인 유사도 구하기
        df_similarity = pd.DataFrame({'url': columns_index, 'similarity': similarity})
        
        top_15 = df_similarity.sort_values(by='similarity', ascending=False).head(15)
        if top_15.iloc[0]['similarity'] == 0.0:
            print(f'검색하신 키워드를 취급하는 회사는 존재하지 않습니다.')
            return None;
        index_list = []
        for i in range(len(top_15)):  
            url = top_15.iloc[i]['url']
            index_list.append(df[df['URL'] == url].index[0]) #인덱스 값만 가지고 옴 
        
        last_answer = df.iloc[index_list]
        
        index_name = []
        for num in range(len(last_answer)):
            index_name.append(f'{num+1}위')
        last_answer.index= index_name
        return last_answer
    


result=inputKeyword("w")
result


result


json_result = result.to_json(orient="records")
